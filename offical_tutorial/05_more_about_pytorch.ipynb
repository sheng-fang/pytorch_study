{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch provides two main features:\n",
    "\n",
    "1. An n-dimensional Tensor, similar to numpy but can run on GPUs\n",
    "2. Automatic differentiation for building and training neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy implementation of simple neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 29567787.738610502\n",
      "1 24457765.147065353\n",
      "2 22659605.8826482\n",
      "3 20876017.786632977\n",
      "4 17926053.166493855\n",
      "5 13784792.658249406\n",
      "6 9627475.332427816\n",
      "7 6245486.32385573\n",
      "8 3957971.239617762\n",
      "9 2536621.770930306\n",
      "10 1699221.564872202\n",
      "11 1202452.5261491346\n",
      "12 899854.4979643864\n",
      "13 705530.9641489021\n",
      "14 573325.2522519502\n",
      "15 477984.4204233486\n",
      "16 405813.7688010348\n",
      "17 348910.5373758633\n",
      "18 302646.43417220964\n",
      "19 264243.56659488997\n",
      "20 231884.20427245984\n",
      "21 204333.74148629763\n",
      "22 180651.77076087677\n",
      "23 160233.3945872188\n",
      "24 142477.147804356\n",
      "25 126984.78904275344\n",
      "26 113420.79218311023\n",
      "27 101502.03534443589\n",
      "28 91007.38179749422\n",
      "29 81731.11435985743\n",
      "30 73518.33448503519\n",
      "31 66233.57529309057\n",
      "32 59753.16610717938\n",
      "33 53983.1721642143\n",
      "34 48829.96527533239\n",
      "35 44222.529949841315\n",
      "36 40095.36811382796\n",
      "37 36394.1852193477\n",
      "38 33068.98821346843\n",
      "39 30079.300746631227\n",
      "40 27384.50598861856\n",
      "41 24958.714476061643\n",
      "42 22772.344341866203\n",
      "43 20795.2204913157\n",
      "44 19004.905277962065\n",
      "45 17381.204079249394\n",
      "46 15908.143105318792\n",
      "47 14570.615893070164\n",
      "48 13354.031486573418\n",
      "49 12247.511598081073\n",
      "50 11242.243169695435\n",
      "51 10325.66761434712\n",
      "52 9489.499368186056\n",
      "53 8726.377131566409\n",
      "54 8028.907508260481\n",
      "55 7391.280121447266\n",
      "56 6807.763409012545\n",
      "57 6273.591819699186\n",
      "58 5784.161316663356\n",
      "59 5335.4578114434535\n",
      "60 4923.8175681594885\n",
      "61 4545.851215400886\n",
      "62 4199.31674612224\n",
      "63 3880.892887966393\n",
      "64 3587.993051215715\n",
      "65 3318.558322660337\n",
      "66 3070.5702852719955\n",
      "67 2842.2500047954304\n",
      "68 2631.833626896665\n",
      "69 2437.811897637517\n",
      "70 2258.84727168715\n",
      "71 2093.727227437209\n",
      "72 1941.3048991669507\n",
      "73 1800.591654882104\n",
      "74 1670.5413039139285\n",
      "75 1550.450888295893\n",
      "76 1439.559133555082\n",
      "77 1336.9622327867305\n",
      "78 1242.07090772341\n",
      "79 1154.1689948871694\n",
      "80 1072.7867969248737\n",
      "81 997.4182026473933\n",
      "82 927.5788471220098\n",
      "83 862.8687681538115\n",
      "84 802.8537800979591\n",
      "85 747.1947128660973\n",
      "86 695.5993833989235\n",
      "87 647.7339006545546\n",
      "88 603.3124498293334\n",
      "89 562.0764085023729\n",
      "90 523.7635663083718\n",
      "91 488.18871460909264\n",
      "92 455.11928474138966\n",
      "93 424.3780038631322\n",
      "94 395.7985790000473\n",
      "95 369.211820842267\n",
      "96 344.4702480202764\n",
      "97 321.45435322160813\n",
      "98 300.02837820283355\n",
      "99 280.07526638825794\n",
      "100 261.4999115493237\n",
      "101 244.19215219549335\n",
      "102 228.0654456644939\n",
      "103 213.03921669031774\n",
      "104 199.03476087617707\n",
      "105 185.98578875026013\n",
      "106 173.8201067313055\n",
      "107 162.47036299202261\n",
      "108 151.88027568690512\n",
      "109 142.00140926752672\n",
      "110 132.78290932711823\n",
      "111 124.18369419575164\n",
      "112 116.1591748277844\n",
      "113 108.67204711045943\n",
      "114 101.68054218729233\n",
      "115 95.1520214208412\n",
      "116 89.05467893452851\n",
      "117 83.35803258732764\n",
      "118 78.03612331376138\n",
      "119 73.06316539590334\n",
      "120 68.41464071681585\n",
      "121 64.07053542256979\n",
      "122 60.00890702256867\n",
      "123 56.20973010844422\n",
      "124 52.65670607319844\n",
      "125 49.335217288392656\n",
      "126 46.22729208253013\n",
      "127 43.320301528309685\n",
      "128 40.600332729867944\n",
      "129 38.05396917382901\n",
      "130 35.67141849435836\n",
      "131 33.44159397620787\n",
      "132 31.354673178571346\n",
      "133 29.40058069786788\n",
      "134 27.570533416723748\n",
      "135 25.857735033741662\n",
      "136 24.253381779214813\n",
      "137 22.750911107043905\n",
      "138 21.343438299863745\n",
      "139 20.025112706179137\n",
      "140 18.789422151987786\n",
      "141 17.63134753776509\n",
      "142 16.546332906916774\n",
      "143 15.529412219462818\n",
      "144 14.576342862434371\n",
      "145 13.683038480228287\n",
      "146 12.845368209563294\n",
      "147 12.059764915105074\n",
      "148 11.323578243567512\n",
      "149 10.633057010563395\n",
      "150 9.98515192417863\n",
      "151 9.377617012472964\n",
      "152 8.807621401738874\n",
      "153 8.272950001729187\n",
      "154 7.771277604408739\n",
      "155 7.300531384205913\n",
      "156 6.858803598285134\n",
      "157 6.444445239463238\n",
      "158 6.055418258908646\n",
      "159 5.690390610928771\n",
      "160 5.347682784141363\n",
      "161 5.026055975458598\n",
      "162 4.723909796058191\n",
      "163 4.440266435942676\n",
      "164 4.1739575353452345\n",
      "165 3.9237747255417714\n",
      "166 3.688889989104896\n",
      "167 3.4683066327780905\n",
      "168 3.2610503608846804\n",
      "169 3.066508612632493\n",
      "170 2.8836928306656424\n",
      "171 2.711922513523386\n",
      "172 2.5505543590913047\n",
      "173 2.3990531579528613\n",
      "174 2.2566181693697387\n",
      "175 2.122748349983535\n",
      "176 1.9969174030921457\n",
      "177 1.8786713958206769\n",
      "178 1.7675118624338486\n",
      "179 1.6630552062025974\n",
      "180 1.5649021298957146\n",
      "181 1.4726241361063246\n",
      "182 1.385840854717526\n",
      "183 1.3042384274815375\n",
      "184 1.2275081048365473\n",
      "185 1.1554093052189724\n",
      "186 1.0875635855814296\n",
      "187 1.0237765482160817\n",
      "188 0.9637821885679587\n",
      "189 0.9073542628540144\n",
      "190 0.8542601719436266\n",
      "191 0.804324832750844\n",
      "192 0.7573412272113867\n",
      "193 0.7131530238100047\n",
      "194 0.6715856369046957\n",
      "195 0.6324670352331073\n",
      "196 0.5956808955577358\n",
      "197 0.5610497080671861\n",
      "198 0.5284536567144216\n",
      "199 0.4977764645132718\n",
      "200 0.46890318747693144\n",
      "201 0.44173117324187827\n",
      "202 0.41615729645947097\n",
      "203 0.39207949335472386\n",
      "204 0.369416460468458\n",
      "205 0.34808764912943724\n",
      "206 0.3279990386458399\n",
      "207 0.3090994119760455\n",
      "208 0.291289583906292\n",
      "209 0.2745246388928133\n",
      "210 0.2587348185785656\n",
      "211 0.24386250590324776\n",
      "212 0.22985901002648504\n",
      "213 0.21667282176307526\n",
      "214 0.2042497831519829\n",
      "215 0.19254837120733925\n",
      "216 0.1815268514368648\n",
      "217 0.1711520263027064\n",
      "218 0.1613729140671067\n",
      "219 0.15215806493927564\n",
      "220 0.14347618359695252\n",
      "221 0.13529834381383155\n",
      "222 0.12759046114685493\n",
      "223 0.12032684868189746\n",
      "224 0.11348245270493429\n",
      "225 0.10703396492176133\n",
      "226 0.10095521636609239\n",
      "227 0.09522952116511196\n",
      "228 0.08983142725993215\n",
      "229 0.08474312156952962\n",
      "230 0.07994602794941594\n",
      "231 0.07542353001548778\n",
      "232 0.07116054205119382\n",
      "233 0.0671417258294099\n",
      "234 0.06335212224437595\n",
      "235 0.05977917893251176\n",
      "236 0.05641068990361038\n",
      "237 0.05323711125977502\n",
      "238 0.05024216887756136\n",
      "239 0.0474176755450777\n",
      "240 0.04475397996393173\n",
      "241 0.04224232299051135\n",
      "242 0.03987328654655542\n",
      "243 0.0376381445283467\n",
      "244 0.03553001473799687\n",
      "245 0.033541655256303184\n",
      "246 0.03166692289382571\n",
      "247 0.029898124003714027\n",
      "248 0.02822867595880605\n",
      "249 0.02665411094233286\n",
      "250 0.025167866829022115\n",
      "251 0.02376588068052173\n",
      "252 0.022442780804621434\n",
      "253 0.02119442831686848\n",
      "254 0.020016702675779693\n",
      "255 0.018905170334881607\n",
      "256 0.017856501337523485\n",
      "257 0.01686647984597672\n",
      "258 0.015931901421495503\n",
      "259 0.015049590416239336\n",
      "260 0.014216745332583798\n",
      "261 0.01343067603617239\n",
      "262 0.012688837249258681\n",
      "263 0.011988510018125148\n",
      "264 0.011327094434228697\n",
      "265 0.01070298923958446\n",
      "266 0.010113798233639802\n",
      "267 0.009557238863267828\n",
      "268 0.009031538585733266\n",
      "269 0.00853513509040756\n",
      "270 0.008066396171841293\n",
      "271 0.0076238991676634334\n",
      "272 0.007205938877423575\n",
      "273 0.0068110657197871126\n",
      "274 0.0064383457293006405\n",
      "275 0.006086167297925015\n",
      "276 0.00575345624670633\n",
      "277 0.005439216763792205\n",
      "278 0.005142314374491864\n",
      "279 0.0048618014323280135\n",
      "280 0.004596930536017272\n",
      "281 0.004346564065852781\n",
      "282 0.004110034867493111\n",
      "283 0.0038866829774593147\n",
      "284 0.0036754764050718155\n",
      "285 0.0034759160753120187\n",
      "286 0.003287315993030178\n",
      "287 0.0031090992724256425\n",
      "288 0.0029406519392887985\n",
      "289 0.002781562600182544\n",
      "290 0.0026311381393440197\n",
      "291 0.002488941240972353\n",
      "292 0.0023545783413480353\n",
      "293 0.002227492597065458\n",
      "294 0.0021073741601768795\n",
      "295 0.001993827396508424\n",
      "296 0.0018864614485427495\n",
      "297 0.001784950862063977\n",
      "298 0.001689016444908189\n",
      "299 0.0015982914792332134\n",
      "300 0.001512513226946747\n",
      "301 0.001431404879089596\n",
      "302 0.0013546877166963657\n",
      "303 0.0012821293299185071\n",
      "304 0.0012135261181650055\n",
      "305 0.0011486235916945793\n",
      "306 0.0010872441361498815\n",
      "307 0.0010292057117703892\n",
      "308 0.0009743012593651317\n",
      "309 0.0009223891254192607\n",
      "310 0.0008732607843716552\n",
      "311 0.0008267729264299011\n",
      "312 0.0007827913552651621\n",
      "313 0.0007411855246439687\n",
      "314 0.0007018347172155898\n",
      "315 0.0006645906915838501\n",
      "316 0.0006293519471068658\n",
      "317 0.0005960237966557587\n",
      "318 0.000564462482271947\n",
      "319 0.0005346106729323887\n",
      "320 0.0005063486832886936\n",
      "321 0.0004795991209455709\n",
      "322 0.0004542823645693903\n",
      "323 0.00043032289818115154\n",
      "324 0.0004076472434008993\n",
      "325 0.00038618537610303927\n",
      "326 0.00036587770966882026\n",
      "327 0.0003466431954541644\n",
      "328 0.0003284294859673886\n",
      "329 0.00031119181492575404\n",
      "330 0.00029486808132089565\n",
      "331 0.0002794120797970694\n",
      "332 0.00026477590245076036\n",
      "333 0.00025092073270765005\n",
      "334 0.0002378012861925689\n",
      "335 0.0002253787208410347\n",
      "336 0.00021361401409197644\n",
      "337 0.00020246758660814288\n",
      "338 0.0001919130560076795\n",
      "339 0.00018191892361965427\n",
      "340 0.00017245037997070707\n",
      "341 0.00016348170263110273\n",
      "342 0.0001549857302608674\n",
      "343 0.0001469383901875185\n",
      "344 0.0001393134535357777\n",
      "345 0.00013209281244313726\n",
      "346 0.0001252501106133223\n",
      "347 0.00011876616183068603\n",
      "348 0.00011262189997948933\n",
      "349 0.00010680135825433216\n",
      "350 0.00010128783521526236\n",
      "351 9.606053147760585e-05\n",
      "352 9.110607536104269e-05\n",
      "353 8.641069706086962e-05\n",
      "354 8.19610217955351e-05\n",
      "355 7.774581105842867e-05\n",
      "356 7.37503676364699e-05\n",
      "357 6.996185375440824e-05\n",
      "358 6.63722772933808e-05\n",
      "359 6.29681765170724e-05\n",
      "360 5.974055122466455e-05\n",
      "361 5.668159158950808e-05\n",
      "362 5.378107908369156e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363 5.103116891708409e-05\n",
      "364 4.842356281712531e-05\n",
      "365 4.595142561641726e-05\n",
      "366 4.360915090324755e-05\n",
      "367 4.13862063469515e-05\n",
      "368 3.9277815785248995e-05\n",
      "369 3.7278466337585265e-05\n",
      "370 3.5382615996263405e-05\n",
      "371 3.358438049182483e-05\n",
      "372 3.187959300429893e-05\n",
      "373 3.026186873948641e-05\n",
      "374 2.872779167098572e-05\n",
      "375 2.7272276769362002e-05\n",
      "376 2.5891959648536885e-05\n",
      "377 2.458234439674e-05\n",
      "378 2.3339833557484153e-05\n",
      "379 2.216077908640371e-05\n",
      "380 2.1042222952467133e-05\n",
      "381 1.9980925388783588e-05\n",
      "382 1.8974060005472825e-05\n",
      "383 1.8018642000848312e-05\n",
      "384 1.711203950042868e-05\n",
      "385 1.6251574190151847e-05\n",
      "386 1.543495275499156e-05\n",
      "387 1.466028908662597e-05\n",
      "388 1.3924799978571695e-05\n",
      "389 1.322678041080549e-05\n",
      "390 1.2564220408224946e-05\n",
      "391 1.193524683757781e-05\n",
      "392 1.1338150961045892e-05\n",
      "393 1.0771325506880275e-05\n",
      "394 1.0233283064218375e-05\n",
      "395 9.722606159873932e-06\n",
      "396 9.237682106487305e-06\n",
      "397 8.777392819580389e-06\n",
      "398 8.340499105188244e-06\n",
      "399 7.925386446577256e-06\n",
      "400 7.531250167373526e-06\n",
      "401 7.157059204884738e-06\n",
      "402 6.801589324929175e-06\n",
      "403 6.46404977109609e-06\n",
      "404 6.143515456562064e-06\n",
      "405 5.839052057991797e-06\n",
      "406 5.549973693862662e-06\n",
      "407 5.275412406609003e-06\n",
      "408 5.014663200827753e-06\n",
      "409 4.766938523534431e-06\n",
      "410 4.531550282265524e-06\n",
      "411 4.3079510801406715e-06\n",
      "412 4.095567475513984e-06\n",
      "413 3.893761282063819e-06\n",
      "414 3.702048544550873e-06\n",
      "415 3.519919166793111e-06\n",
      "416 3.3468389508802823e-06\n",
      "417 3.182395780221964e-06\n",
      "418 3.026191170149453e-06\n",
      "419 2.8777369994413445e-06\n",
      "420 2.7367000002662525e-06\n",
      "421 2.602586275870875e-06\n",
      "422 2.475131057509699e-06\n",
      "423 2.3540319961813572e-06\n",
      "424 2.2389559427008187e-06\n",
      "425 2.1295370656552048e-06\n",
      "426 2.0255430428983636e-06\n",
      "427 1.9266886227541208e-06\n",
      "428 1.8327182543449158e-06\n",
      "429 1.7434282972296405e-06\n",
      "430 1.6585403466557089e-06\n",
      "431 1.5778364596711309e-06\n",
      "432 1.501127701816866e-06\n",
      "433 1.4281621045173676e-06\n",
      "434 1.3587806553600147e-06\n",
      "435 1.2928223885574553e-06\n",
      "436 1.2301216481492386e-06\n",
      "437 1.1704780079481674e-06\n",
      "438 1.113766095133199e-06\n",
      "439 1.0598518553452152e-06\n",
      "440 1.0085737119966944e-06\n",
      "441 9.598116975532812e-07\n",
      "442 9.134413190394765e-07\n",
      "443 8.693489848902725e-07\n",
      "444 8.273891770041936e-07\n",
      "445 7.874764324471503e-07\n",
      "446 7.495140651964897e-07\n",
      "447 7.134056690372349e-07\n",
      "448 6.790747361346651e-07\n",
      "449 6.464110316521051e-07\n",
      "450 6.153278566879537e-07\n",
      "451 5.85754947525049e-07\n",
      "452 5.576229958298706e-07\n",
      "453 5.308568825664839e-07\n",
      "454 5.054014847975012e-07\n",
      "455 4.81188463352482e-07\n",
      "456 4.581326401842359e-07\n",
      "457 4.3619757474173393e-07\n",
      "458 4.1533125916366367e-07\n",
      "459 3.9546618172008293e-07\n",
      "460 3.765676657876435e-07\n",
      "461 3.585785122049876e-07\n",
      "462 3.4145954402603613e-07\n",
      "463 3.251657385186815e-07\n",
      "464 3.096579001251844e-07\n",
      "465 2.9489837720999277e-07\n",
      "466 2.808567600310025e-07\n",
      "467 2.674988963269417e-07\n",
      "468 2.5477118025593655e-07\n",
      "469 2.4265736497287647e-07\n",
      "470 2.3112470234891797e-07\n",
      "471 2.2014595299728315e-07\n",
      "472 2.0969597158656299e-07\n",
      "473 1.997472550980126e-07\n",
      "474 1.9027431763359855e-07\n",
      "475 1.812583123269724e-07\n",
      "476 1.7267361205464252e-07\n",
      "477 1.6449815950798134e-07\n",
      "478 1.5671445253526263e-07\n",
      "479 1.493075505798946e-07\n",
      "480 1.4224999156983156e-07\n",
      "481 1.355289535008007e-07\n",
      "482 1.2912873049527238e-07\n",
      "483 1.2303596298152871e-07\n",
      "484 1.172326923468125e-07\n",
      "485 1.1170725439187429e-07\n",
      "486 1.0644426095270491e-07\n",
      "487 1.0143032222014543e-07\n",
      "488 9.665532611080417e-08\n",
      "489 9.210726428366244e-08\n",
      "490 8.777547376110029e-08\n",
      "491 8.365217817261265e-08\n",
      "492 7.972395953147924e-08\n",
      "493 7.598000910433575e-08\n",
      "494 7.241343885354503e-08\n",
      "495 6.901623888034768e-08\n",
      "496 6.578028426019991e-08\n",
      "497 6.26975621703451e-08\n",
      "498 5.976080506761408e-08\n",
      "499 5.696310501296929e-08\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behind the scenes, Tensors can keep track of a computational graph and gradients, but theyâ€™re also useful as a generic tool for scientific computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 645.451171875\n",
      "199 4.03122091293335\n",
      "299 0.038073353469371796\n",
      "399 0.0006547098746523261\n",
      "499 6.61577214486897e-05\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 402.6940612792969\n",
      "199 2.1221063137054443\n",
      "299 0.026554986834526062\n",
      "399 0.0006959157763049006\n",
      "499 9.091220272239298e-05\n"
     ]
    }
   ],
   "source": [
    "for t in range(500):\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define new autogrd functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyReLU(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tenors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input<0] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch high level API torch.nn, similar to Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1.8192226886749268\n",
      "199 0.02704755589365959\n",
      "299 0.0008998168050311506\n",
      "399 4.299845022615045e-05\n",
      "499 2.5010826902871486e-06\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. Each Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    model.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pytorch: optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
